{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## *Парсинг* названий научных статей с одной из популярных онлайн библиотек.\n",
        "\n",
        "#### Для примера произведен отбор **20 тыс.** заголовков, из которых сформирован DataFrame.\n",
        "\n",
        "#### Полученный DataFrame предназначен для проверки работы модели **multiT5-3tasks-titles_scientific_articles.pth**\n"
      ],
      "metadata": {
        "id": "umIKrTFwy97J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAV4xi0xy85X"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekULpprvy85c"
      },
      "outputs": [],
      "source": [
        "main_url = 'https://cyberleninka.ru' # На мой взгляд, лучший российский онлайн ресурс в своем сегменте. Браво создателям!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1xM_cyGy85e"
      },
      "source": [
        "#### Для начала напишем функцию для определения количества страниц"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRmtiQ9wy85g"
      },
      "outputs": [],
      "source": [
        "def get_number_of_pages(main_url, base_link):\n",
        "    new_url = main_url + base_link\n",
        "    req = requests.get(new_url).text\n",
        "    soup = BeautifulSoup(req, 'html.parser')\n",
        "\n",
        "    link_last_page = soup.find('a', class_='icon').get('href').split('/')\n",
        "    last_page = [int(el) for el in link_last_page if el.isdigit()]\n",
        "    return last_page[-1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Далее напишем основную функцию по парсингу названий научных статей"
      ],
      "metadata": {
        "id": "PCwuUgsZ0dSk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGbPGSm1y85i"
      },
      "outputs": [],
      "source": [
        "def titles_scientific_articles(main_url, pages):\n",
        "    titles_list = []  # Список для хранения названий статей\n",
        "    session = requests.Session()  # Создаем сессию для повторного использования соединений\n",
        "\n",
        "    req = session.get(main_url).text\n",
        "    soup = BeautifulSoup(req, 'html.parser')\n",
        "\n",
        "    targets = soup.find_all('div', class_=['half', 'half-right'])\n",
        "\n",
        "    count = 0  # Счетчик добавленных названий статей\n",
        "    start_time = time.time()\n",
        "\n",
        "    for el in targets:\n",
        "        direction = el.find_all('a')\n",
        "\n",
        "        for article in direction:\n",
        "            base_link = article.get('href')  # Получаем базовый href\n",
        "\n",
        "            if base_link is None:  # Проверяем, что link не None\n",
        "                continue\n",
        "\n",
        "            if pages == 'all':\n",
        "                number_of_pages = get_number_of_pages(main_url, base_link)\n",
        "                time.sleep(0.25)\n",
        "            else:\n",
        "                number_of_pages = pages\n",
        "\n",
        "            for page in range(1, number_of_pages + 1):  # Включаем последнюю страницу\n",
        "                new_url = main_url + base_link + f'/{page}'  # Формируем новый URL\n",
        "\n",
        "                try:\n",
        "                    new_soup = BeautifulSoup(session.get(new_url).text, 'html.parser')\n",
        "                    target_lst = new_soup.find_all('div', class_='title')\n",
        "                    for name in target_lst:\n",
        "                        titles_list.append(name.text)  # Добавляем название статьи в список\n",
        "                        count += 1  # Увеличиваем счетчик\n",
        "\n",
        "                        # Проверяем, достигли ли мы 20000 добавленных названий\n",
        "                        if count >= 20000:\n",
        "                            print(f\"Добавлено {count} статей. Завершение функции.\")\n",
        "                            titles_df = pd.DataFrame(titles_list, columns=['name'])  # Создаем DataFrame один раз из списка\n",
        "                            return titles_df.reset_index(drop=True)\n",
        "\n",
        "                        # Каждые 1000 статей выводим информацию о процессе\n",
        "                        if count % 1000 == 0:\n",
        "                            print(f\"{count} статей внесено в список.\")\n",
        "                            execution_time = time.time() - start_time\n",
        "                            print(f\"Время выполнения {count} строк: {execution_time:.2f} секунд\")\n",
        "\n",
        "                        time.sleep(0.25)  # Задержка между запросами\n",
        "                except Exception as e:\n",
        "                    print(f\"Ошибка при обработке {new_url}: {e}\")\n",
        "\n",
        "    titles_df = pd.DataFrame(titles_list, columns=['name'])  # Создаем DataFrame один раз из списка\n",
        "    return titles_df.reset_index(drop=True)\n",
        "\n",
        "# Вызов функции\n",
        "titles_df = titles_scientific_articles(main_url, pages='all')\n",
        "print(titles_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVPhYbDty85j"
      },
      "outputs": [],
      "source": [
        "titles_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zm-cFZf5y85k"
      },
      "outputs": [],
      "source": [
        "titles_df.to_csv(\"titles_scientific_articles_Leninka.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}